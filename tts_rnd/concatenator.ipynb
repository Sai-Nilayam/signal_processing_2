{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95bec9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining imports and constants. \n",
    "\n",
    "# Imports\n",
    "# Required imports are at it's place where it is needed to be used. However here, there should be a list of all \n",
    "# imports so that it will be possible to use those just by runnig the 1st shell.\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from itertools import product\n",
    "import pyrubberband\n",
    "\n",
    "# Constants\n",
    "END = '\\n\\n'\n",
    "SAMPLE_RATE = 384000\n",
    "# SAMPLE_RATE = 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e949078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable\n",
    "\n",
    "# All english alphabets\n",
    "# a b c-> d e f g h i j k l m n o p q- r s t u v w- x- y- z\n",
    "\n",
    "# Stands for Vyanjanavarna set.\n",
    "vv_set = [\n",
    "    'k', 'kh', 'g', 'gh', 'n2',\n",
    "    'ch', 'chh', 'j', 'jh',\n",
    "    't', 'th', 'd', 'dh',\n",
    "    't2', 'th2', 'd2', 'dh2',\n",
    "    'p', 'ph', 'b', 'bh',\n",
    "    \n",
    "    'f', 'v', 'z',\n",
    "    \n",
    "    'y', 'w', 'q', 'x',\n",
    "    \n",
    "    # A part of the vv is pvv too.\n",
    "    'r', 'l', 'sh', 's', 'h',\n",
    "    'n', 'm',\n",
    "]\n",
    "\n",
    "pvv_set = [\n",
    "    'r', 'l', 'sh', 's',\n",
    "    'n', 'm',\n",
    "    'g', 'j', 'd', 'd2', 'b',\n",
    "    'gh', 'jh', 'dh', 'dh2', 'bh',\n",
    "]\n",
    "\n",
    "# Stands for Swaravarna set.\n",
    "sv_set = [\n",
    "    'a', 'a2', 'i', 'u', 'e', 'o',\n",
    "#     'ae', 'ao',\n",
    "    # All these mixed sv are not required in this set. Insted we are going to collect these from real large text\n",
    "    # corpuses.\n",
    "#     'aa2', 'ai', 'au', 'ae', 'ao',\n",
    "#     'a2a', 'a2i', 'a2u', 'a2e', 'a2o',\n",
    "#     'ia', 'ia2', 'iu', 'ie', 'io',\n",
    "#     'ua', 'ua2', 'ui', 'ue', 'uo',\n",
    "#     'ea', 'ea2', 'ei', 'eu', 'eo',\n",
    "#     'oa', 'oa2', 'oi', 'ou', 'oe',\n",
    "    \n",
    "    # Extra Mappings. \n",
    "    'aa'\n",
    "]\n",
    "\n",
    "# Stands for Punctuation set. \n",
    "p_set = [\n",
    "    ',', '.', '?', '!'\n",
    "]\n",
    "\n",
    "char_set = sv_set + vv_set + p_set\n",
    "\n",
    "sv_set_large = [\n",
    "    # Here we are going to append all the new sv those are collected from large text corpus. \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8162015b",
   "metadata": {},
   "source": [
    "### Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbde279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 'a', 'e', 'k', 't2', 'e', 'sh']\n"
     ]
    }
   ],
   "source": [
    "# This shell is responsible for taking a word as input and converting it to it's character list.\n",
    "\n",
    "# input: 'ankush'\n",
    "# output: ['a', 'n', 'k', 'u', 'sh']\n",
    "\n",
    "def word_to_chars(word, char_1, char_2, char_3, char_4):\n",
    "    # We will be taking 4 different lists.\n",
    "    word_to_char_list = []\n",
    "\n",
    "    i = 0\n",
    "    final_index = len(word)\n",
    "    \n",
    "    # Writing the word_to_char_list formatter script.\n",
    "    while i < final_index:\n",
    "        # Check if the character is in the char_4.\n",
    "        try:\n",
    "            inst_char = word[i : i+4]\n",
    "            if inst_char in char_4:\n",
    "                word_to_char_list.append(inst_char)\n",
    "                i = i + 4\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Check if the character is in the char_3.\n",
    "        try:\n",
    "            inst_char = word[i : i+3]\n",
    "            if inst_char in char_3:\n",
    "                word_to_char_list.append(inst_char)\n",
    "                i = i + 3\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Check if the character is in the char_2.\n",
    "        try:\n",
    "            inst_char = word[i : i+2]\n",
    "            if inst_char in char_2:\n",
    "                if inst_char == 'aa':\n",
    "                    word_to_char_list.append('a2')\n",
    "                else:\n",
    "                    word_to_char_list.append(inst_char)\n",
    "                i = i + 2\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Check if the character is in the char_1.\n",
    "        try:\n",
    "            inst_char = word[i : i+1]\n",
    "            if inst_char in char_1:\n",
    "                word_to_char_list.append(inst_char)\n",
    "                i = i + 1\n",
    "                continue\n",
    "            else:\n",
    "                # If the character was not found anywhere append '_na_' for it.\n",
    "                word_to_char_list.append('_na_')\n",
    "                i = i + 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return word_to_char_list\n",
    "\n",
    "# Using the function.\n",
    "word = 'raekt2esh'\n",
    "\n",
    "char_1 = []\n",
    "char_2 = []\n",
    "char_3 = []\n",
    "char_4 = []\n",
    "\n",
    "for char in char_set:\n",
    "    if len(char) == 4:\n",
    "        char_4.append(char)\n",
    "    if len(char) == 3:\n",
    "        char_3.append(char)\n",
    "    if len(char) == 2:\n",
    "        char_2.append(char)\n",
    "    if len(char) == 1:\n",
    "        char_1.append(char)\n",
    "\n",
    "# print(char_1, end=END)\n",
    "# print(char_2, end=END)\n",
    "# print(char_3, end=END)\n",
    "# print(char_4, end=END)\n",
    "\n",
    "word_to_char_list = word_to_chars(word, char_1, char_2, char_3, char_4)\n",
    "print(word_to_char_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a9ab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 'a', 'e', 'k', 't2', 'e', 'sh']\n",
      "['r', 'ae', 'k', 't2', 'e', 'sh']\n"
     ]
    }
   ],
   "source": [
    "# This shell is responsible for merging the swaravarnas given a word_to_char_list.\n",
    "\n",
    "def word_to_connected_chars(word_to_char_list):\n",
    "    word_to_connected_char_list = []\n",
    "\n",
    "    inst_sv = ''\n",
    "\n",
    "    for i in range(len(word_to_char_list)):\n",
    "        inst_char = word_to_char_list[i]\n",
    "\n",
    "        if inst_char in sv_set:\n",
    "            inst_sv = inst_sv + inst_char\n",
    "\n",
    "        if inst_char in vv_set:\n",
    "            if inst_sv != '':\n",
    "                word_to_connected_char_list.append(inst_sv)\n",
    "                # Also append it in the sv_set_large.\n",
    "                sv_set_large.append(inst_sv)\n",
    "                inst_sv = ''\n",
    "\n",
    "            word_to_connected_char_list.append(inst_char)\n",
    "\n",
    "        if i == len(word_to_char_list) - 1:\n",
    "            if inst_sv != '':\n",
    "                word_to_connected_char_list.append(inst_sv)\n",
    "                # Also append it in sv_set_large\n",
    "                sv_set_large.append(inst_sv)\n",
    "                \n",
    "        # Append any punctuation if it's present.\n",
    "        if inst_char in [',', '.', '?', '!']:\n",
    "            word_to_connected_char_list.append(inst_char)\n",
    "                \n",
    "    return word_to_connected_char_list\n",
    "        \n",
    "word = 'raekt2esh'\n",
    "word_to_char_list = word_to_chars(word, char_1, char_2, char_3, char_4)\n",
    "print(word_to_char_list)\n",
    "\n",
    "word_to_connected_char_list = word_to_connected_chars(word_to_char_list)\n",
    "print(word_to_connected_char_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c3f0e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'r', 'a', 'r', 't', 'o', 't', 'p', 'r', 'o']\n",
      "\n",
      "['t', 'r', 'a', 'r', 't', 'o', 't', 'p', 'r', 'o']\n",
      "\n",
      "['t_s', 'r_s', 'a_s', 'r_e_s', '_g_', 't_s', 'o_m', 't_e', '_g_', 'p_s', 'r_s', 'o_e']\n",
      "\n",
      "['t_s', 'r_s', 'a_s', 'r_e_s', '_g_', 't_s', 'o_m', 't_e', '_g_', 'p_s', 'r_s', 'o_e']\n"
     ]
    }
   ],
   "source": [
    "# This shell is responsible for creating the fluctuated character list out of the main character list.\n",
    "\n",
    "def char_fluctuator(word_to_char_list):\n",
    "    chars = word_to_connected_char_list\n",
    "    # print(chars, end=END)\n",
    "\n",
    "    char_flct = []\n",
    "\n",
    "    for i in range(len(chars)):\n",
    "        char = chars[i]\n",
    "        if char in vv_set:\n",
    "            try:\n",
    "                char_after = chars[i + 1]\n",
    "                char_before = chars[i - 1]\n",
    "                if char_after in sv_set_large:\n",
    "                    char_flct.append(char + '_s')\n",
    "                    \n",
    "                elif (char_before in sv_set_large) and (i != 0):\n",
    "                    char_flct.append(char + '_e')\n",
    "                    char_flct.append('_g_')\n",
    "                else:\n",
    "                    char_flct.append(char + '_s')\n",
    "            except:\n",
    "                pass\n",
    "            if i == len(chars)-1:\n",
    "                char_flct.append(char + '_e')\n",
    "        elif char in sv_set_large:\n",
    "            char_flct.append(char)\n",
    "\n",
    "    # print(char_flct, end=END)\n",
    "\n",
    "    # Now reversing the char_flct\n",
    "    char_flct.reverse()\n",
    "    # print(char_flct, end=END)\n",
    "\n",
    "    char_flct_final = []\n",
    "\n",
    "    j = 0\n",
    "    for i in range(len(char_flct)):\n",
    "        char = char_flct[i]\n",
    "\n",
    "        if char in sv_set_large:\n",
    "            # Check if ending SV.\n",
    "            is_ending_sv = True\n",
    "            for k in range(i+1, len(char_flct)):\n",
    "                char_nexts = char_flct[k]\n",
    "                if char_nexts in sv_set_large:\n",
    "                    is_ending_sv = False\n",
    "\n",
    "            if j == 0:\n",
    "                char_flct_final.append(char + '_e')\n",
    "                j = j + 1\n",
    "\n",
    "            elif j > 1 and is_ending_sv == True:\n",
    "                char_flct_final.append(char + '_s')\n",
    "\n",
    "            elif j > 0:\n",
    "                char_flct_final.append(char + '_m')\n",
    "                j = j + 1\n",
    "        else:\n",
    "            char_flct_final.append(char)\n",
    "\n",
    "    char_flct_final.reverse()\n",
    "\n",
    "    # print(char_flct_final, end=END)\n",
    "\n",
    "    # Now it's time to handle the pvv.\n",
    "    char_flct_list_altimate = []\n",
    "\n",
    "    for i in range(len(char_flct_final)):\n",
    "        char_flct = char_flct_final[i]\n",
    "        char = char_flct.split('_')[0]\n",
    "        char_suffix = char_flct.split('_')[1]\n",
    "        if char in pvv_set and char_suffix == 'e':\n",
    "            try:\n",
    "                char_before = char_flct_final[i - 1]\n",
    "                char_before_suffix = char_before.split('_')[1]\n",
    "                char_before_pure = char_before.split('_')[0]\n",
    "                if char_before_pure in sv_set_large:\n",
    "                    char_flct_list_altimate.append(char_flct + '_' + char_before_suffix)\n",
    "                else:\n",
    "                    char_flct_list_altimate.append(char_flct + '_e')\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            char_flct_list_altimate.append(char_flct)\n",
    "            \n",
    "    print(char_flct_list_altimate, end=END)\n",
    "    \n",
    "    # Now it's time to handle the speacial 'h' sound when it comes.\n",
    "    char_flct_list_altimate_2 = []\n",
    "        \n",
    "    for i in range(len(char_flct_list_altimate)):\n",
    "        char_flct = char_flct_list_altimate[i]\n",
    "        char = char_flct.split('_')[0]\n",
    "        char_flct_suffix = char_flct.split('_')[1]\n",
    "        \n",
    "        if (char_flct_suffix == 's') and (char[-1] == 'h' or char[-2:] == 'h2') and (char != 'ch'):\n",
    "            if char == 'h':\n",
    "                char_split_1 = 'h'\n",
    "            elif char[-2:] == 'h2':\n",
    "                char_split_1 = char[0: len(char)-2] + '2'\n",
    "            else:\n",
    "                char_split_1 = char[0 : len(char)-1]\n",
    "            \n",
    "            char_next = char_flct_list_altimate[i + 1]\n",
    "            char_next_pure = char_next.split('_')[0]\n",
    "            \n",
    "            if (char_next_pure in sv_set) or (char_next_pure in sv_set_large):\n",
    "                first_letter = char_next_pure[0]\n",
    "                needed_h = 'h_s_' + first_letter\n",
    "                if char != 'h':\n",
    "                    char_flct_list_altimate_2.append(char_split_1 + '_s')\n",
    "                    char_flct_list_altimate_2.append(needed_h)\n",
    "                else:\n",
    "                    char_flct_list_altimate_2.append(needed_h)\n",
    "            else:\n",
    "                char_flct_list_altimate_2.append(char_split_1)\n",
    "                char_flct_list_altimate_2.append('h_s_a')\n",
    "            \n",
    "        elif char_flct == 'h_e':\n",
    "            before_char = char_flct_list_altimate[i - 1]\n",
    "            before_char_pure = before_char.split('_')[0]\n",
    "            if (before_char_pure in sv_set) or (before_char_pure in sv_set_large):\n",
    "                last_letter = before_char_pure[-1]\n",
    "                needed_h = 'h_e_' + last_letter\n",
    "                char_flct_list_altimate_2.append(needed_h)\n",
    "            else:\n",
    "                char_flct_list_altimate_2.append(char_flct)\n",
    "        else:\n",
    "            char_flct_list_altimate_2.append(char_flct)\n",
    "    \n",
    "    return char_flct_list_altimate_2\n",
    "\n",
    "\n",
    "\n",
    "# Using the function.\n",
    "word = 'trartotpro'\n",
    "\n",
    "word_to_char_list = word_to_chars(word, char_1, char_2, char_3, char_4)\n",
    "print(word_to_char_list, end=END)\n",
    "\n",
    "word_to_connected_char_list = word_to_connected_chars(word_to_char_list)\n",
    "print(word_to_connected_char_list, end=END)\n",
    "\n",
    "char_flct_list = char_fluctuator(word_to_connected_char_list)\n",
    "print(char_flct_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb68ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character_list: ['m', 'u', 's', 'k', 'a2', 'n']\n",
      "\n",
      "connected_character_list: ['m', 'u', 's', 'k', 'a2', 'n']\n",
      "\n",
      "['m_s', 'u_m', 's_e_m', '_g_', 'k_s', 'a2_e', 'n_e_e']\n",
      "\n",
      "char_flct_list ['m_s', 'u_m', 's_e_m', '_g_', 'k_s', 'a2_e', 'n_e_e']\n",
      "\n",
      "not_existed_charaters: []\n",
      "\n",
      "concatenation_case: ['m_s', 'u_m', 's_e_m', '_g_', 'k_s', 'a2_e', 'n_e_e']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4889/2481814172.py:146: FutureWarning: Pass y=[-0.00094604 -0.00094604 -0.00100708 ...  0.00085449  0.00076294\n",
      "  0.00085449] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  actual_length = librosa.get_duration(speech_ar, sr=SAMPLE_RATE)\n",
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Guessed Channel Layout for Input Stream #0.0 : mono\n",
      "Input #0, wav, from '/home/sainilayam/Desktop/dev/project_21/tts_rnd/outputs/word.wav':\n",
      "  Duration: 00:00:00.96, bitrate: 6144 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 384000 Hz, mono, s16, 6144 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/home/sainilayam/Desktop/dev/project_21/tts_rnd/outputs/word_sc.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 384000 Hz, mono, s16, 6144 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=       0kB time=00:00:00.00 bitrate=N/A speed=N/A    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word saved with file name 'word.wav' in the outputs folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "size=     526kB time=00:00:00.68 bitrate=6306.0kbits/s speed=8.76x    \n",
      "video:0kB audio:525kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.018959%\n"
     ]
    }
   ],
   "source": [
    "# This shell is responsible for converting char_flct_list to it's speech form.\n",
    "\n",
    "# input: ['a_s', 'n_e', '_g_', 'k_s', 'u_m', 'sh_e']\n",
    "# output: 'word.wav'\n",
    "\n",
    "def char_flct_to_speech(char_flct_list, noise, vv_set_name, sv_set_name, pvv_set_name, vv_volume, sv_volume, yukt_gap=0.2):\n",
    "    # Beofore putting any gaps we need to import the noise and crop the part as gap as per the gap time\n",
    "    # specified. \n",
    "    noise_ar, sr = librosa.load('data/noise_set/{}.wav'.format(noise), sr=SAMPLE_RATE)\n",
    "    \n",
    "    _g_ = noise_ar[: int(yukt_gap * SAMPLE_RATE)]\n",
    "\n",
    "    speech_ar = np.array([])\n",
    "    \n",
    "    concatenation_case = []\n",
    "\n",
    "    for i in range(len(char_flct_list)):\n",
    "        inst_char = char_flct_list[i]\n",
    "        inst_char_pure = inst_char.split('_')[0]\n",
    "        inst_char_suffix = inst_char.split('_')[1]\n",
    "\n",
    "        if (inst_char_pure in vv_set):\n",
    "            if inst_char_pure in pvv_set and inst_char_suffix == 'e' :\n",
    "                inst_chunk_ar, inst_sr = librosa.load('data/sv_sets/{}/{}/{}'.format(sv_set_name, pvv_set_name, inst_char + '.wav'), sr = SAMPLE_RATE) \n",
    "            else:\n",
    "                inst_chunk_ar, inst_sr = librosa.load('data/vv_sets/{}/{}'.format(vv_set_name, inst_char + '.wav'), sr=SAMPLE_RATE)\n",
    "                inst_chunk_ar = inst_chunk_ar * vv_volume\n",
    "            speech_ar = np.concatenate([speech_ar, inst_chunk_ar])\n",
    "            concatenation_case.append(inst_char)\n",
    "        \n",
    "        if inst_char == '_g_':\n",
    "            inst_chunk_ar = _g_\n",
    "            speech_ar = np.concatenate([speech_ar, inst_chunk_ar])\n",
    "            concatenation_case.append(inst_char)\n",
    "        \n",
    "        if inst_char_pure in sv_set_large:\n",
    "            # Handling the concatenation of the sv is a bit different.\n",
    "            try:\n",
    "                char_after = char_flct_list[i + 1]\n",
    "                char_after_pure = char_after.split('_')[0]\n",
    "                char_after_pure_suffix = char_after.split('_')[1]\n",
    "                \n",
    "                if char_after_pure in vv_set:\n",
    "                    if char_after_pure in pvv_set and char_after_pure_suffix == 'e':\n",
    "                        char_after_ar, inst_sr = librosa.load('data/sv_sets/{}/{}/{}'.format(sv_set_name, pvv_set_name, char_after + '.wav'), sr = SAMPLE_RATE)\n",
    "                    else:\n",
    "                        char_after_ar, inst_sr = librosa.load('data/vv_sets/{}/{}'.format(vv_set_name, char_after + '.wav'), sr=SAMPLE_RATE)\n",
    "                        \n",
    "                    inst_chunk_ar, inst_sr = librosa.load('data/sv_sets/{}/{}'.format(sv_set_name, inst_char + '.wav'), sr=SAMPLE_RATE)\n",
    "                    \n",
    "                    char_after_len = len(char_after_ar)\n",
    "                    inst_chunk_len = len(inst_chunk_ar)\n",
    "                    \n",
    "                    # To see if the it's taking unsusual concatenation.\n",
    "                    # print(inst_chunk_len, char_after_len)\n",
    "                    \n",
    "                    if inst_chunk_len >char_after_len:\n",
    "                        inst_chunk_ar_appendable = inst_chunk_ar[: inst_chunk_len - char_after_len]\n",
    "                        speech_ar = np.concatenate([speech_ar, inst_chunk_ar_appendable])\n",
    "                        concatenation_case.append(inst_char)\n",
    "                    else:\n",
    "                        concatenation_case.append('unsual_concatenation_susp')\n",
    "                else:\n",
    "                    inst_chunk_ar, inst_sr = librosa.load('data/sv_sets/{}/{}'.format(sv_set_name, inst_char + '.wav'), sr=SAMPLE_RATE)\n",
    "                    inst_chunk_ar = inst_chunk_ar * sv_volume\n",
    "                    speech_ar = np.concatenate([speech_ar, inst_chunk_ar])\n",
    "                    concatenation_case.append(inst_char)\n",
    "            except:\n",
    "                inst_chunk_ar, inst_sr = librosa.load('data/sv_sets/{}/{}'.format(sv_set_name, inst_char + '.wav'), sr=SAMPLE_RATE)\n",
    "                inst_chunk_ar = inst_chunk_ar * sv_volume\n",
    "                speech_ar = np.concatenate([speech_ar, inst_chunk_ar])\n",
    "#                 concatenation_case.append(inst_char)\n",
    "                \n",
    "    print('concatenation_case:', concatenation_case)\n",
    "    return speech_ar\n",
    "\n",
    "# vv_l = 'k'\n",
    "# word = vv_l + 'a' + vv_l + 'aa' + vv_l + 'i' + vv_l + 'u' + vv_l + 'e' + vv_l + 'o'\n",
    "\n",
    "word = 'muskaan'\n",
    "\n",
    "word_to_char_list = word_to_chars(word, char_1, char_2, char_3, char_4)\n",
    "print('character_list:', word_to_char_list, end=END)\n",
    "\n",
    "word_to_connected_char_list = word_to_connected_chars(word_to_char_list)\n",
    "print('connected_character_list:', word_to_connected_char_list, end=END)\n",
    "\n",
    "char_flct_list = char_fluctuator(word_to_connected_char_list)\n",
    "print('char_flct_list', char_flct_list, end=END)\n",
    "\n",
    "noise = 'noise_n2'\n",
    "vv_set_name = 'vv_set_n2_t2'\n",
    "sv_set_name = 'sv_set_n2_t2_s2_revised'\n",
    "pvv_set_name = 'pvv_set_n2_t2_s2_revised'\n",
    "yukt_gap = 0.001\n",
    "vv_volume = 1\n",
    "sv_volume = 1\n",
    "\n",
    "# It become true if all the characters needed for a word pronunciation exist in a folder. \n",
    "is_ready = False\n",
    "\n",
    "existed_vv = os.listdir('data/vv_sets/{}/'.format(vv_set_name))\n",
    "existed_vv = [vv.split('.')[0] for vv in existed_vv]\n",
    "# print(existed_vv, end=END)\n",
    "\n",
    "existed_sv = os.listdir('data/sv_sets/{}/'.format(sv_set_name))\n",
    "existed_sv = [sv.split('.')[0] for sv in existed_sv]\n",
    "# print(existed_sv, end=END)\n",
    "\n",
    "existed_pvv = os.listdir('data/sv_sets/{}/{}/'.format(sv_set_name, pvv_set_name))\n",
    "existed_pvv = [pvv.split('.')[0] for pvv in existed_pvv]\n",
    "not_exist_chars = []\n",
    "\n",
    "for char in char_flct_list:\n",
    "    if char == '_g_':\n",
    "        pass\n",
    "    elif char in existed_vv:\n",
    "        pass\n",
    "    elif char in existed_sv:\n",
    "        pass\n",
    "    elif char in existed_pvv:\n",
    "        pass\n",
    "    else:\n",
    "        not_exist_chars.append(char)\n",
    "        \n",
    "print('not_existed_charaters:', not_exist_chars, end=END)\n",
    "\n",
    "if len(not_exist_chars) == 0:\n",
    "    is_ready = True\n",
    "\n",
    "# Only execute the audio concatenation if all the chunks are available.\n",
    "if is_ready == True:\n",
    "    speech_ar = char_flct_to_speech(char_flct_list, noise, vv_set_name, sv_set_name, pvv_set_name, vv_volume, sv_volume, yukt_gap)\n",
    "    \n",
    "    sf.write('outputs/word.wav', speech_ar, SAMPLE_RATE, format='wav')\n",
    "    \n",
    "    # Now chaning the pitch of the speech.\n",
    "#     speed = 0.5\n",
    "#     path_to_word = '/home/sainilayam/Desktop/dev/project_21/tts_rnd/outputs/word.wav'\n",
    "#     path_to_output = '/home/sainilayam/Desktop/dev/project_21/tts_rnd/outputs/word_sc.wav'\n",
    "#     cmd = 'ffmpeg -i {} -filter:a \"atempo={}\" -vn {} -y'.format(path_to_word, speed, path_to_output)\n",
    "#     os.system(cmd)\n",
    "    \n",
    "    # Now chaning the speed of the speech, as per expectation.\n",
    "    expected_length = 0.7\n",
    "    actual_length = librosa.get_duration(speech_ar, sr=SAMPLE_RATE)\n",
    "    speed_index = actual_length / expected_length\n",
    "    \n",
    "    path_to_word = '/home/sainilayam/Desktop/dev/project_21/tts_rnd/outputs/word.wav'\n",
    "    path_to_output = '/home/sainilayam/Desktop/dev/project_21/tts_rnd/outputs/word_sc.wav'\n",
    "    cmd = 'ffmpeg -i {} -filter:a \"atempo={}\" -vn {} -y'.format(path_to_word, speed_index, path_to_output)\n",
    "    os.system(cmd)\n",
    "    \n",
    "    # Printing out the message.\n",
    "    print('Word saved with file name \\'word.wav\\' in the outputs folder.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4fde6c",
   "metadata": {},
   "source": [
    "### So this point is a break over the whole TTS pipeline. If we only want to pronounce variables, then all we need to do is to work up to here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071de43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sainilayam/personal/dev/venvs/tf/lib/python3.10/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m_s', 'u_m', 's_e_m', '_g_', 'k_s', 'a2_e', 'n_e_e']\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/sv_sets/sv_set_n2_t2_s2_revised/0.4/s_e_m.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/personal/dev/venvs/tf/lib/python3.10/site-packages/librosa/core/audio.py:164\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m~/personal/dev/venvs/tf/lib/python3.10/site-packages/librosa/core/audio.py:195\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/personal/dev/venvs/tf/lib/python3.10/site-packages/soundfile.py:740\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    739\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/personal/dev/venvs/tf/lib/python3.10/site-packages/soundfile.py:1264\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file: \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m-> 1264\u001b[0m \u001b[43m_error_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_snd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msf_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_ptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError opening \u001b[39;49m\u001b[38;5;132;43;01m{0!r}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "File \u001b[0;32m~/personal/dev/venvs/tf/lib/python3.10/site-packages/soundfile.py:1455\u001b[0m, in \u001b[0;36m_error_check\u001b[0;34m(err, prefix)\u001b[0m\n\u001b[1;32m   1454\u001b[0m err_str \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error_number(err)\n\u001b[0;32m-> 1455\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(prefix \u001b[38;5;241m+\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mstring(err_str)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error opening 'data/sv_sets/sv_set_n2_t2_s2_revised/0.4/s_e_m.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m word_gap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00\u001b[39m\n\u001b[1;32m     57\u001b[0m comma_gap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00\u001b[39m\n\u001b[0;32m---> 59\u001b[0m sentence_ar \u001b[38;5;241m=\u001b[39m \u001b[43msentence_to_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_gap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomma_gap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Implementing a sentence level time stretching algorithm.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# sentence_ar = librosa.effects.time_stretch(sentence_ar, rate=1.5)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m sf\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/sentence.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence_ar, SAMPLE_RATE)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36msentence_to_speech\u001b[0;34m(sentence, noise, word_gap, comma_gap)\u001b[0m\n\u001b[1;32m     25\u001b[0m         word_to_connected_char_list \u001b[38;5;241m=\u001b[39m word_to_connected_chars(word_to_char_list)\n\u001b[1;32m     26\u001b[0m         char_flct_list \u001b[38;5;241m=\u001b[39m char_fluctuator(word_to_connected_char_list)\n\u001b[0;32m---> 27\u001b[0m         word_ar \u001b[38;5;241m=\u001b[39m \u001b[43mchar_flct_to_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar_flct_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvv_set_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msv_set_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvv_volume\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msv_volume\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myukt_gap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         sentence_ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([sentence_ar, word_ar, _wg_])\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Adding a little extra part at the end of the clip.\u001b[39;00m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mchar_flct_to_speech\u001b[0;34m(char_flct_list, noise, vv_set_name, sv_set_name, pvv_set_name, vv_volume, sv_volume, yukt_gap)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (inst_char_pure \u001b[38;5;129;01min\u001b[39;00m vv_set):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst_char_pure \u001b[38;5;129;01min\u001b[39;00m pvv_set \u001b[38;5;129;01mand\u001b[39;00m inst_char_suffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m :\n\u001b[0;32m---> 24\u001b[0m         inst_chunk_ar, inst_sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/sv_sets/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msv_set_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpvv_set_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst_char\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSAMPLE_RATE\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m         inst_chunk_ar, inst_sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/vv_sets/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(vv_set_name, inst_char \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m), sr\u001b[38;5;241m=\u001b[39mSAMPLE_RATE)\n",
      "File \u001b[0;32m~/personal/dev/venvs/tf/lib/python3.10/site-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     94\u001b[0m ]\n",
      "File \u001b[0;32m~/personal/dev/venvs/tf/lib/python3.10/site-packages/librosa/core/audio.py:170\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    169\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/personal/dev/venvs/tf/lib/python3.10/site-packages/librosa/core/audio.py:226\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    223\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    229\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/personal/dev/venvs/tf/lib/python3.10/site-packages/audioread/__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/personal/dev/venvs/tf/lib/python3.10/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/sv_sets/sv_set_n2_t2_s2_revised/0.4/s_e_m.wav'"
     ]
    }
   ],
   "source": [
    "# This shell is responsible for taking a sentence as input and convert it to it's speech form. \n",
    "\n",
    "# Now here we are going to write a function to pronouce a sentence. \n",
    "def sentence_to_speech(sentence, noise, word_gap, comma_gap):\n",
    "    word_list = sentence.split(' ')\n",
    "    \n",
    "    noise_ar, sr = librosa.load('data/noise_set/{}.wav'.format(noise), sr=SAMPLE_RATE)\n",
    "\n",
    "    _wg_ = noise_ar[: int(word_gap * SAMPLE_RATE)]\n",
    "    _cg_ = noise_ar[: int(comma_gap * SAMPLE_RATE)]\n",
    "\n",
    "    sentence_ar = np.array([])\n",
    "    \n",
    "    # Adding a little extra part before the ar.\n",
    "    sentence_ar = np.concatenate([sentence_ar, noise_ar[: int(0.4 * SAMPLE_RATE)]])\n",
    "    \n",
    "    # Adding a little bit of gap at the beginning of the clip. \n",
    "    sentence_ar = np.concatenate([sentence_ar, _wg_])\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word == ',':\n",
    "            sentence_ar = np.concatenate([sentence_ar, _cg_])\n",
    "        else:\n",
    "            word_to_char_list = word_to_chars(word, char_1, char_2, char_3, char_4)\n",
    "            word_to_connected_char_list = word_to_connected_chars(word_to_char_list)\n",
    "            char_flct_list = char_fluctuator(word_to_connected_char_list)\n",
    "            word_ar = char_flct_to_speech(char_flct_list, noise, vv_set_name, sv_set_name, vv_volume, sv_volume, yukt_gap)\n",
    "            sentence_ar = np.concatenate([sentence_ar, word_ar, _wg_])\n",
    "    \n",
    "    # Adding a little extra part at the end of the clip.\n",
    "    sentence_ar = np.concatenate([sentence_ar, noise_ar[: int(0.4 * SAMPLE_RATE)]])\n",
    "    \n",
    "    return sentence_ar\n",
    "\n",
    "sentence = 'gaghinkugh'\n",
    "# sentence = 't2um hama2re dost2ho.' \n",
    "\n",
    "noise = 'noise_n2'\n",
    "vv_set_name = 'vv_set_n2_t2'\n",
    "sv_set_name = 'sv_set_n2_t2_s2_revised'\n",
    "pvv_set_name = 'pvv_set_n2_t2_s2_revised'\n",
    "\n",
    "yukt_gap = 0.02\n",
    "\n",
    "# Repeatitiong of vv\n",
    "# vv_repeat = int()\n",
    "# This thing will be managed by ration implemenation. \n",
    "\n",
    "# This thing will be managed with accent transfer with starting middle and ending sounds.\n",
    "# If Swara comes at the end then fade out.\n",
    "# sv_fadeout = float()\n",
    "\n",
    "vv_volume = 0.4\n",
    "sv_volume = 1\n",
    "\n",
    "word_gap = 0.00\n",
    "comma_gap = 0.00\n",
    "\n",
    "sentence_ar = sentence_to_speech(sentence, noise, word_gap, comma_gap)\n",
    "# Implementing a sentence level time stretching algorithm.\n",
    "# sentence_ar = librosa.effects.time_stretch(sentence_ar, rate=1.5)\n",
    "\n",
    "sf.write('outputs/sentence.wav', sentence_ar, SAMPLE_RATE)\n",
    "\n",
    "# Printing out the status message.\n",
    "print('Sentence saved with file name \\'sentence.wav\\' in the outputs folder.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35f87d",
   "metadata": {},
   "source": [
    "### Extra  Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shell is responsible for taking all the Swaravarnas from a sv_set_name folder and create _s, _m, _e \n",
    "# sounds from the main file and save them all in the same folder.\n",
    "\n",
    "def create_psd_flct_sv(sv_set_name):\n",
    "#     sv_set_name = 'sv_set_n3_t1_s1'\n",
    "    sv_set_path = 'data/sv_sets/' + sv_set_name + '/'\n",
    "\n",
    "    file_list = os.listdir(sv_set_path)\n",
    "\n",
    "    for i in range(len(file_list)):\n",
    "        inst_file = file_list[i]\n",
    "        \n",
    "        # Check if the Varna exists in sv_set to prevent remake of files those are already processed.\n",
    "        inst_file_varna = inst_file.split('.')[0]\n",
    "        \n",
    "        if inst_file_varna in sv_set:\n",
    "            inst_file_source_path = sv_set_path + inst_file\n",
    "\n",
    "            inst_file_destiantion_path = sv_set_path + inst_file.split('.')[0] + '_s.wav'\n",
    "            shutil.copy(inst_file_source_path, inst_file_destiantion_path)\n",
    "\n",
    "            inst_file_destiantion_path = sv_set_path + inst_file.split('.')[0] + '_m.wav'\n",
    "            shutil.copy(inst_file_source_path, inst_file_destiantion_path)\n",
    "\n",
    "            inst_file_destiantion_path = sv_set_path + inst_file.split('.')[0] + '_e.wav'\n",
    "            shutil.copy(inst_file_source_path, inst_file_destiantion_path)\n",
    "\n",
    "    # Writing out the message status.\n",
    "    print('All files with the name _s, _m, _e created at the destination folder.')\n",
    "    \n",
    "# Using the function.\n",
    "sv_set_name = 'sv_set_n6_t1_s1'\n",
    "create_psd_flct_sv(sv_set_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46210220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shell is responsible for taking all the Vyanjana Varnas from a vv_set_name folder and create _s, _e \n",
    "# sounds from the main file and save them all in the same folder.\n",
    "\n",
    "def create_psd_flct_vv(vv_set_name):\n",
    "#     vv_set_name = 'vv_set_n1_t1'\n",
    "    vv_set_path = 'data/vv_sets/' + vv_set_name + '/'\n",
    "\n",
    "    file_list = os.listdir(vv_set_path)\n",
    "\n",
    "    for i in range(len(file_list)):\n",
    "        inst_file = file_list[i]\n",
    "        \n",
    "        # Check if the Varna exists in sv_set to prevent remake of files those are already processed.\n",
    "        inst_file_varna = inst_file.split('.')[0].split('_')[0]\n",
    "        \n",
    "        if inst_file_varna in vv_set:\n",
    "            inst_file_source_path = vv_set_path + inst_file\n",
    "\n",
    "#             inst_file_destiantion_path = sv_set_path + inst_file.split('.')[0] + '_s.wav'\n",
    "#             shutil.copy(inst_file_source_path, inst_file_destiantion_path)\n",
    "\n",
    "#             inst_file_destiantion_path = sv_set_path + inst_file.split('.')[0] + '_m.wav'\n",
    "#             shutil.copy(inst_file_source_path, inst_file_destiantion_path)\n",
    "\n",
    "            inst_file_destiantion_path = vv_set_path + inst_file.split('.')[0].split('_')[0] + '_e.wav'\n",
    "            shutil.copy(inst_file_source_path, inst_file_destiantion_path)\n",
    "\n",
    "    # Writing out the message status.\n",
    "    print('All files with the name _s, _e created at the destination folder.')\n",
    "    \n",
    "# Using the function.\n",
    "vv_set_name = 'vv_set_n1_t1'\n",
    "create_psd_flct_vv(vv_set_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shell is responsible for creating a k Replced sentence from a normal sentence.\n",
    "\n",
    "# input: 'di em a2e fainans me a2pka2 sua2gat2 hae.'\n",
    "# output: 'ki e_na_ a2e _na_aikakk _na_e a2kka2 kua2kak kae.'\n",
    "# further output: 'i e_na_ a2e _na_aia _na_e a2a2 ua2a ae.'\n",
    "\n",
    "# The reason we are doing this is to pronouce the whole sentece with 'k' as Vyanjana Varna and then we can \n",
    "# pronouce the sentence without 'k' to get the pure Swaravarna sounds with the way these should be pronouced. \n",
    "\n",
    "# Then we need a script that will only keep the Swaravarnas, spaces, ',', '.', '?', '!'  and removes all other \n",
    "# Vyanjavarnas with 'k'.\n",
    "\n",
    "def k_replaced_sentence_maker(text):\n",
    "    text_ar = text.split(' ')\n",
    "    \n",
    "    text_k_ar = [] \n",
    "\n",
    "    for i in range(len(text_ar)):\n",
    "        inst_word = text_ar[i]\n",
    "        inst_word_to_char_ar = word_to_chars(inst_word, char_1, char_2, char_3, char_4)\n",
    "\n",
    "        for j in range(len(inst_word_to_char_ar)):\n",
    "            inst_char = inst_word_to_char_ar[j]\n",
    "            if inst_char in vv_set:\n",
    "                inst_word_to_char_ar[j] = 'k'\n",
    "\n",
    "        inst_word = ''.join(inst_word_to_char_ar)\n",
    "        text_k_ar.append(inst_word)\n",
    "\n",
    "    text_k = ' '.join(text_k_ar)\n",
    "    \n",
    "    return text_k\n",
    "\n",
    "# sentence = 'di em a2e fainans me a2pka2 sua2gat2 hae.'\n",
    "sentence = 'diepha2epe a2pka2 sua2gat2hae.'\n",
    "\n",
    "print(sentence, end=END)\n",
    "\n",
    "sentence_k = k_replaced_sentence_maker(sentence)\n",
    "print(sentence_k, end=END)\n",
    "\n",
    "sentence_sv = sentence_k.replace('k', '')\n",
    "print(sentence_sv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b49a89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This shell is reposible for taking input a Vyanjana Varna and test with all different Swaravarnas to see check\n",
    "# the quality of that Vyanjanavarna crop.\n",
    "\n",
    "# input: 'k'\n",
    "# output: A clip with the sounds 'ka', 'ka2', 'ki', 'ku', 'ke', 'ko'\n",
    "\n",
    "def create_sv_vv_test_clip(vv, sv_list):\n",
    "    # vv = 'k_s'\n",
    "    # sv_list = ['a', 'a2', 'i', 'u', 'e', 'o']\n",
    "\n",
    "    noise = 'noise_n2'\n",
    "    vv_set = 'vv_set_n2_t1'\n",
    "    sv_set = 'sv_set_n2_t1_s1'\n",
    "\n",
    "    # Importing the noise.\n",
    "    noise_path = 'data/noise_set/' + noise + '.wav'\n",
    "    noise_ar, sr = librosa.load(noise_path, sr=SAMPLE_RATE)\n",
    "    \n",
    "    gap_ar = noise_ar[: int(0.05 * SAMPLE_RATE)]\n",
    "    \n",
    "    # Improting the vv.\n",
    "    vv_path = 'data/vv_sets/' + vv_set + '/' + vv + '.wav'\n",
    "    vv_ar, sr = librosa.load(vv_path, sr=SAMPLE_RATE) \n",
    "\n",
    "    vv_sv_ar = np.array([])\n",
    "    # Adding a 0.4 second gap at the beginning of the ar.\n",
    "    vv_sv_ar = np.concatenate([vv_sv_ar, noise_ar[: int(0.4 * SAMPLE_RATE)]])\n",
    "\n",
    "    # Now we need to write the concatenation loop.\n",
    "    for i in range(len(sv_list)):\n",
    "        inst_sv = sv_list[i]\n",
    "        inst_sv_path = 'data/sv_sets/' + sv_set + '/' + inst_sv + '.wav'\n",
    "        inst_sv_ar, sr = librosa.load(inst_sv_path, sr=SAMPLE_RATE)\n",
    "        \n",
    "        # Detecting if the Vyanjanavarna is starting or the ending sound.\n",
    "        inst_sign = vv.split('_')[1]\n",
    "        \n",
    "        if inst_sign == 's':\n",
    "            vv_sv_ar = np.concatenate([vv_sv_ar, vv_ar, inst_sv_ar, noise_ar[: int(0.4 * SAMPLE_RATE)]])\n",
    "        if inst_sign == 'e':\n",
    "            vv_sv_ar = np.concatenate([vv_sv_ar, inst_sv_ar, vv_ar, noise_ar[: int(0.4 * SAMPLE_RATE)]])\n",
    "        \n",
    "    # Now writing the ar.\n",
    "    sf.write('outputs/vv_sv.wav', vv_sv_ar, SAMPLE_RATE)\n",
    "    # Showing the status message.\n",
    "    print('Outputs are generated at the output folder with name \\'vv_sv.wav\\'.')\n",
    "\n",
    "# Using the function.\n",
    "vv = 'k_s'\n",
    "sv_list = ['a', 'a2', 'i', 'u', 'e', 'o']\n",
    "create_sv_vv_test_clip(vv, sv_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b957fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shell is responsible for outputting all the underlying information of a sentence w.r.t how the TTS\n",
    "# pipeline is working.\n",
    "\n",
    "# input: ''\n",
    "# output_1: ''\n",
    "# output_2: ''\n",
    "# output_3: ''\n",
    "\n",
    "def get_pipeline_info(sentence):\n",
    "    # sentence = 'di em a2e fainans me a2pka2 sua2gat2 hae.'\n",
    "    print('sentence:', sentence, end=END)\n",
    "\n",
    "    word_list = sentence.split(' ') \n",
    "\n",
    "    sentence_to_connected_char_list = []\n",
    "\n",
    "    for word in word_list:\n",
    "        inst_word_to_char_list = word_to_chars(word, char_1, char_2, char_3, char_4)\n",
    "        inst_word_to_connected_char_list = word_to_connected_chars(inst_word_to_char_list)\n",
    "        sentence_to_connected_char_list.append(inst_word_to_connected_char_list)\n",
    "    print('sentence_to_chars:',sentence_to_connected_char_list, end=END)\n",
    "    \n",
    "    sentence_char_flct_list = []\n",
    "\n",
    "    for char_list in sentence_to_connected_char_list:\n",
    "        inst_char_flct_list = char_fluctuator(char_list)\n",
    "        sentence_char_flct_list.append(inst_char_flct_list)\n",
    "    print('sentence_to_char_flct:', sentence_char_flct_list, end=END)\n",
    "\n",
    "    sentece_vv_flct_list = []\n",
    "\n",
    "    sentence_sv_flct_list = []\n",
    "    sentence_sv_list = []\n",
    "\n",
    "    for char_flct_list in sentence_char_flct_list:\n",
    "\n",
    "        for char in char_flct_list:\n",
    "            inst_char_pure = char.split('_')[0]\n",
    "\n",
    "            if inst_char_pure in vv_set:\n",
    "                sentece_vv_flct_list.append(char)\n",
    "\n",
    "            if inst_char_pure in sv_set_large:\n",
    "                sentence_sv_flct_list.append(char)\n",
    "                sentence_sv_list.append(inst_char_pure)\n",
    "\n",
    "    sentece_vv_flct_list_unique = list(dict.fromkeys(sentece_vv_flct_list))\n",
    "\n",
    "    sentence_sv_flct_list_unique = list(dict.fromkeys(sentence_sv_flct_list))\n",
    "    sentence_sv_list_unique = list(dict.fromkeys(sentence_sv_list))\n",
    "\n",
    "    print('sentence_vv_flct_unique:', sentece_vv_flct_list_unique, end=END)\n",
    "    print('length_of_vv_flct_uniqe:', len(sentence_sv_flct_list_unique), end=END)\n",
    "    print('setnce_sv_flct_unique:', sentence_sv_flct_list_unique, end=END)\n",
    "    print('sentce_sv_unique:', sentence_sv_list_unique, end=END)\n",
    "    print('length_of_sv_unique:', len(sentence_sv_list_unique), end=END)\n",
    "    \n",
    "    # Returning sentence_to_char_flct.\n",
    "    return sentence_char_flct_list\n",
    "    \n",
    "# sentence = 'di em a2e fainans me a2pka2 sua2gat2 hae.'\n",
    "# sentence = 'diepha2epe a2pka2 sua2gat2hae.'\n",
    "sentence = 'a2pjobhiho, achheho.'\n",
    "\n",
    "get_pipeline_info(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a197b8c8",
   "metadata": {},
   "source": [
    "### Procedure to build the TTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First take your whole text. \n",
    "# 2. Write it in the way it should be according to the script notation that this TTS follow. Also feel free to \n",
    "# update the notations. Remember not to mess up with any previous notation.\n",
    "# 3. Use proper commas after each phrases properly.\n",
    "# 4. Use the information section to get what are different vv and sv needed for the TTS to run.\n",
    "# 5. For different persons, take vvs in different tones.\n",
    "# 6. For different styles go with different sv sounds. The vv set used here is going to be of the same tone \n",
    "# in the tone it should be \n",
    "# 7. For the sv sounds, crop it from real word pronunciations.\n",
    "# 8. Once you get a _s, _m, _e sound of one length sv, you can just mimic exactly the same for other vvs as\n",
    "# well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229142c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shell is responsible for taking a list of sentences as input and give information of those sentnces\n",
    "# iteratively. You can also get the infromation of the whole text.\n",
    "\n",
    "# All Setnences taken in to consideration.\n",
    "# --------\n",
    "sentences = [\n",
    "#     'a2pka2 ham, kia2, mad2at2, kar sakt2e, haen?',\n",
    "#     'bat2aie, a2p kaese ho?',\n",
    "#     'hama2re pa2s, isse jia2da2 bata2ne ke lie, kuchh nahi hae',\n",
    "#     'paribart2an sansa2rka2 niam hae.',\n",
    "#     'di em a2e fa2inans me, a2pka2, sua2gat hae.',\n",
    "    'ham sab, ek haen.',\n",
    "    't2um hama2re, dost2 ho.',\n",
    "#     'ra2z a2nkhen t2eri, sab baea2n kar rahi.',\n",
    "#     'a2pki lon ki ra2shi, pacha2s rupae hae.',\n",
    "]\n",
    "# --------\n",
    "\n",
    "# Convert the comma separated parts of the sentece to phrases by making thsese as words.\n",
    "sentence_index = 0\n",
    "\n",
    "whole_text = ''\n",
    "for sentence in sentences:\n",
    "    whole_text = whole_text + sentence + ','\n",
    "\n",
    "# --------\n",
    "# Swtich to get information iteratively or as a whole. Switch here by doing commenting and uncommenting. \n",
    "# sentence = sentences[sentence_index]\n",
    "sentence = whole_text\n",
    "# --------\n",
    "\n",
    "print('sentence:', sentence, end=END)\n",
    "\n",
    "sentence_space_removed = sentence.replace(' ', '')\n",
    "sentence_comma_splitted = sentence_space_removed.split(',')\n",
    "\n",
    "# Creating the sentence with proper phrases.\n",
    "sentence_phrased = ' '.join(sentence_comma_splitted)\n",
    "print('sentece_phrased:', sentence_phrased, end=END)\n",
    "\n",
    "# Getting the sentence info.\n",
    "print('Sentece Informations: ', end='\\n\\n')\n",
    "sentence_to_char_flct = get_pipeline_info(sentence_phrased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857059c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shell is responsible for taking a sv_- as input and show a word that contains it, so that we can pronouce\n",
    "# that word and crop it from there. \n",
    "\n",
    "sv_flct = 'a_s'\n",
    "\n",
    "for i in range(len(sentence_to_char_flct)):\n",
    "    inst_char_flct = sentence_to_char_flct[i]\n",
    "    \n",
    "    if sv_flct in inst_char_flct:\n",
    "        try:\n",
    "            inst_char_flct.remove('_g_')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        phrase = ''\n",
    "        for char_flct in inst_char_flct:\n",
    "            if char_flct != sv_flct:\n",
    "                inst_char_pure = char_flct.split('_')[0]\n",
    "                phrase = phrase + inst_char_pure\n",
    "            else:\n",
    "                phrase = phrase + '-' + char_flct + '-'\n",
    "        \n",
    "        print(inst_char_flct)\n",
    "        \n",
    "        print(phrase)\n",
    "        \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shell is responsible for changing the sample rate of any wav file to a different sample rate.\n",
    "\n",
    "file_path = '/home/sainilayam/personal/inst/ranjan.wav'\n",
    "target_file_name = '/home/sainilayam/personal/inst/{}'.format('resampled.wav')\n",
    "target_sample_rate = 8000\n",
    "\n",
    "clip_ar, sr = librosa.load(file_path, sr=target_sample_rate)\n",
    "sf.write(target_file_name, clip_ar, target_sample_rate)\n",
    "\n",
    "print('Resampled file created at target location.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee29f53",
   "metadata": {},
   "source": [
    "### Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b356a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shell is responsible for finding a the length of a wav file give the path to it. \n",
    "\n",
    "path_to_wav = '/home/sainilayam/personal/dev/project_21/tts_rnd/data/sv_sets/sv_set_n2_t2_s2/pvv_set_n2_t2_s2/n_e_e.wav'\n",
    "\n",
    "ar, sr = librosa.load(path_to_wav, sr=SAMPLE_RATE)\n",
    "ar_len = librosa.get_duration(ar, sr=SAMPLE_RATE)\n",
    "\n",
    "print(ar_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shell is responsible for aligning all the similar type of varnas in equal time frame. \n",
    "\n",
    "# len_dict = {\n",
    "#     'sv_s': 0.31,\n",
    "#     'sv_m': 0.34,\n",
    "#     'sv_e': 0.47,\n",
    "    \n",
    "#     'pvv_e_s': 0.25,\n",
    "#     'pvv_e_m': 0.29,\n",
    "#     'pvv_e_e': 0.33\n",
    "# }\n",
    "\n",
    "len_dict = {\n",
    "    'sv_s': 0.31,\n",
    "    'sv_m': 0.28,\n",
    "    'sv_e': 0.47,\n",
    "    \n",
    "    'pvv_e_s': 0.25,\n",
    "    'pvv_e_m': 0.24,\n",
    "    'pvv_e_e': 0.33\n",
    "}\n",
    "\n",
    "\n",
    "sv_path =  '/home/sainilayam/personal/dev/project_21/tts_rnd/data/sv_sets/sv_set_n2_t2_s2/'\n",
    "pvv_path = '/home/sainilayam/personal/dev/project_21/tts_rnd/data/sv_sets/sv_set_n2_t2_s2/' + 'pvv_set_n2_t2_s2/'\n",
    "\n",
    "def grider(len_dict, sv_path, pvv_path):\n",
    "    files_1 = os.listdir(sv_path)\n",
    "    files_2 = os.listdir(pvv_path)\n",
    "\n",
    "    files = files_1 + files_2\n",
    "    files = [file for file in files if file.split('_')[0] != 'pvv']\n",
    "\n",
    "    print(files, end='\\n\\n')\n",
    "    print(len(files))\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        file = files[i]\n",
    "        file_name_pure = file.split('.')[0]\n",
    "        varna_name = file_name_pure.split('_')[0]\n",
    "\n",
    "        if (varna_name in sv_set) or (varna_name in sv_set_large):  \n",
    "            varna_suffix = file_name_pure.split('_')[-1]\n",
    "            if varna_suffix == 's':\n",
    "                expected_len = len_dict['sv_s']\n",
    "            if varna_suffix == 'm':\n",
    "                expected_len = len_dict['sv_m']\n",
    "            if varna_suffix == 'e':\n",
    "                expected_len = len_dict['sv_e']\n",
    "\n",
    "            ar, sr = librosa.load('{}{}'.format(sv_path, file), SAMPLE_RATE)\n",
    "            ar_len = librosa.get_duration(ar, sr=SAMPLE_RATE)\n",
    "\n",
    "            speed_index = ar_len / expected_len\n",
    "            print(speed_index)\n",
    "\n",
    "            path_to_varna = sv_path + file\n",
    "            path_to_output = '/home/sainilayam/personal/dev/project_21/tts_rnd/data/sv_sets/sv_set_revised/'  + file\n",
    "\n",
    "            print(path_to_output)\n",
    "\n",
    "            cmd = 'ffmpeg -i {} -filter:a \"atempo={}\" -vn {} -y'.format(path_to_varna, speed_index, path_to_output)\n",
    "\n",
    "            os.system(cmd)\n",
    "\n",
    "        if varna_name in vv_set:\n",
    "            varna_suffix = file_name_pure.split('_')[-1]\n",
    "            if varna_suffix == 's':\n",
    "                expected_len = len_dict['pvv_e_s']\n",
    "            if varna_suffix == 'm':\n",
    "                expected_len = len_dict['pvv_e_m']\n",
    "            if varna_suffix == 'e':\n",
    "                expected_len = len_dict['pvv_e_e']\n",
    "\n",
    "            ar, sr = librosa.load('{}{}'.format(pvv_path, file), SAMPLE_RATE)\n",
    "            ar_len = librosa.get_duration(ar, sr=SAMPLE_RATE)\n",
    "\n",
    "            speed_index = ar_len / expected_len\n",
    "            print(speed_index)\n",
    "\n",
    "            path_to_varna = pvv_path + file\n",
    "            path_to_output = '/home/sainilayam/personal/dev/project_21/tts_rnd/data/sv_sets/sv_set_revised/pvv_set_revised/'  + file\n",
    "\n",
    "            print(path_to_output)\n",
    "\n",
    "            cmd = 'ffmpeg -i {} -filter:a \"atempo={}\" -vn {} -y'.format(path_to_varna, speed_index, path_to_output)\n",
    "\n",
    "            os.system(cmd)\n",
    "\n",
    "    print('All svs and pvv_e sounds are being properly grided and are placed in proper folders.')\n",
    "\n",
    "# Using the function.\n",
    "grider(len_dict, sv_path, pvv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c337e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shell if responsible for making one _e sound to _e_s, _e_m and _e_e\n",
    "path_to_folder = '/home/sainilayam/personal/dev/project_21/tts_rnd/data/sv_sets/sound_factory/'\n",
    "path_to_output_folder = '/home/sainilayam/personal/dev/project_21/tts_rnd/data/sv_sets/sound_factory_revised/'\n",
    "\n",
    "files = os.listdir(path_to_folder)\n",
    "\n",
    "for i in range(len(files)):\n",
    "    file = files[i]\n",
    "    file_path = path_to_folder + file\n",
    "    \n",
    "    new_file_name = file.split('.')[0] + '_s' + '.wav'\n",
    "    destination_file_path = path_to_output_folder + new_file_name\n",
    "    shutil.copy(file_path, destination_file_path)\n",
    "    \n",
    "    new_file_name = file.split('.')[0] + '_m' + '.wav'\n",
    "    destination_file_path = path_to_output_folder + new_file_name\n",
    "    shutil.copy(file_path, destination_file_path)\n",
    "    \n",
    "    new_file_name = file.split('.')[0] + '_e' + '.wav'\n",
    "    destination_file_path = path_to_output_folder + new_file_name\n",
    "    shutil.copy(file_path, destination_file_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa80e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n",
      "[[240000 264000]\n",
      " [312000 336000]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This shell is responsible for cropping out a bunch of words from a given sound clip.\n",
    "\n",
    "# Creating array that will hold all the words. \n",
    "words_1 = [\n",
    "    'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', \n",
    "    'november', 'december',\n",
    "]\n",
    "\n",
    "words_2 = [\n",
    "    'sunya', 'ek', 'do', 'tin', 'char', 'panch', 'chhe', 'saat', 'aath', 'nao', 'das', 'igyara', 'bara',\n",
    "    'tera', 'chaoda', 'pandra', 'sola', 'satra', 'athara', 'unis', 'bis', \n",
    "]\n",
    "\n",
    "words_3 = [\n",
    "    'ikis', 'bais', 'teis', 'chaobis', 'pachis', 'chhabis', 'satais', 'athais', 'unatis', 'tis', 'ikatis',\n",
    "    'batis', 'taentis', 'chaotis', 'paentis', 'chhatis', 'saentis', 'athtis', 'untalis', 'chalis',\n",
    "]\n",
    "\n",
    "words_4 = [\n",
    "    'iktalis', 'bayalis', 'tiyalis', 'chaowalis', 'paentalis', 'chhialis', 'saentalis', 'athtalis', \n",
    "    'unachas', 'pachas', 'ikyaban', 'baaban', 'tirepan', 'chaoban', 'pachpan', 'chhapan', 'sataban', 'athaban',\n",
    "    'unsath', 'sath',\n",
    "]\n",
    "\n",
    "words_5 = [\n",
    "    'iksath', 'basath', 'tesath', 'chaosath', 'paensath', 'chhiesath', 'sadsath', 'adsath', 'unhtar', 'satar',\n",
    "    'ikatar', 'bahatar', 'tihetar', 'chaohatar', 'pachatar', 'chhihatar', 'satattar', 'athattar', 'unyasi', \n",
    "    'asi',\n",
    "]\n",
    "\n",
    "words_6 = [\n",
    "    'ikyasi', 'bayasi', 'tiyasi', 'chaowasi', 'pachasi', 'chhiasi', 'satasi', 'athasi', 'inyanbe', 'nabe',\n",
    "    'ikyanbe', 'biyanbe', 'tiyanbe', 'chowanbe', 'pachanbe', 'chhiyanbe', 'sataanbe', 'athanbe', 'ninyanbe',\n",
    "]\n",
    "\n",
    "words_7 = [\n",
    "    'sao', 'hazar', 'lakh', 'karod',\n",
    "]\n",
    "\n",
    "words_8 = [\n",
    "    'car_loan', 'house_loan'\n",
    "]\n",
    "\n",
    "words_9 = [\n",
    "    'abhishek'\n",
    "]\n",
    "\n",
    "words_list = words_1\n",
    "\n",
    "# The cropper scirpt.\n",
    "ar_original, sr = librosa.load('words_1.wav', sr=SAMPLE_RATE)\n",
    "ar, sr = librosa.load('words_1.wav', sr=48000)\n",
    "split_indices = librosa.effects.split(ar, top_db=4, frame_length=int(48000 * 0.02), hop_length=int(48000*0.5))\n",
    "print(len(split_indices), end=END)\n",
    "print(split_indices, end=END)\n",
    "\n",
    "for i in range(len(words_list)):\n",
    "    try:\n",
    "        word_name = words_list[i]\n",
    "        file_name = word_name + '.wav'\n",
    "        path = 'chunks/' + file_name\n",
    "\n",
    "        start_index = split_indices[i][0]\n",
    "        start_index_original = int(start_index / 48000) * SAMPLE_RATE\n",
    "\n",
    "        end_index = split_indices[i][1]\n",
    "        end_index_original = int(end_index / 48000) * SAMPLE_RATE\n",
    "\n",
    "        chunk = ar_original[start_index_original : end_index_original]\n",
    "\n",
    "        sf.write(path, chunk, SAMPLE_RATE)\n",
    "    except:\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72c84fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are converted to 8000 Hz.\n"
     ]
    }
   ],
   "source": [
    "# This shell is responsible for converting a whole bunch of audio files to 8000 Hz.\n",
    "\n",
    "data_factory = 'data_factory/'\n",
    "data_factory_output = 'data_factory_output/'\n",
    "\n",
    "files = os.listdir(data_factory)\n",
    "# print(files)\n",
    "\n",
    "for i in range(len(files)):\n",
    "    file = files[i]\n",
    "    input_path = data_factory + file\n",
    "    output_path = data_factory_output + file\n",
    "    \n",
    "    ar, sr = librosa.load(input_path, sr=8000)\n",
    "    sf.write(output_path, ar, 8000)\n",
    "    \n",
    "print('All files are converted to 8000 Hz.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d40232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c082d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc895bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea4d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e2d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a02093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f454e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb21eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49dee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d894b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a514b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ec06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
